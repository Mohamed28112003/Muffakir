{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed28112003/Muffakir/blob/main/Frozen_RAG_%26_HyDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bWmGd91a030"
      },
      "outputs": [],
      "source": [
        "!pip install  transformers\n",
        "!pip install langchain\n",
        "!pip install -U langchain-community\n",
        "!pip install unstructured\n",
        "!pip install \"unstructured[pdf]\"\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install LangChain -q\n",
        "!pip install langchain transformers\n",
        "!pip install chromadb\n",
        "!pip install sentence-transformers\n",
        "!pip install rank-bm25 sentence-transformers\n",
        "!pip uninstall nltk -y\n",
        "!pip install nltk\n",
        "!pip install openai\n",
        "!pip install groq\n",
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pzXFntYq8pys"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "import re\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gt5ufyTkPPq"
      },
      "source": [
        "# Load TXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2-3BXNRVxMkI"
      },
      "outputs": [],
      "source": [
        "data_loader = DirectoryLoader(\"/content/Data\",\n",
        "                             glob=\"*.txt\",\n",
        "                             show_progress=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter_recursive = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # Adjust chunk size based on content length\n",
        "    chunk_overlap=0,  # No overlap needed\n",
        "    separators=[\"Page number\"]  # Split at answer or next question\n",
        ")"
      ],
      "metadata": {
        "id": "G2UdWq4rqFxn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents from the specified directory\n",
        "data = data_loader.load_and_split(text_splitter=text_splitter_recursive) # Load documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaWMKcinqIAg",
        "outputId": "ffeac1b4-c401-4f07-c42c-40b13dd8b6d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:18<00:00, 13.83s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Documents : {len(data)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLa3P6NRRNOr",
        "outputId": "a29cc779-2831-4e2f-f9bd-d138638e00e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents : 3391.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove \"Page number \" from each chunk using regex\n",
        "for i in range(len(data)):\n",
        "  data[i].page_content = re.sub(r\"Page number: \\d+\", \"\", data[i].page_content)"
      ],
      "metadata": {
        "id": "tZnvUF1JrUS2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTfS8mm05aif"
      },
      "source": [
        "# Embeddding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxwZM_CvE0Qy"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the base model\n",
        "base_model = SentenceTransformer('intfloat/multilingual-e5-large')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "\n",
        "path = \"/content/DB\"\n",
        "client = chromadb.PersistentClient(path=path)\n",
        "\n",
        "db_file = client.get_or_create_collection(name='Book',\n",
        "                                          metadata={\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "\n",
        "def embed(texts):\n",
        "    return base_model.encode(texts)\n",
        "\n",
        "i = 0\n",
        "for chunk in data:\n",
        "    embeddings = embed([chunk.page_content])  # Get embeddings using the base model\n",
        "    db_file.add(\n",
        "        documents=[chunk.page_content],\n",
        "        metadatas=[chunk.metadata],\n",
        "        ids=[f\"chunk_{i}\"],\n",
        "        embeddings=embeddings  # Pass the embeddings directly\n",
        "    )\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "Bf7HWs3qumwJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_file.get(ids=[\"chunk_2\"])"
      ],
      "metadata": {
        "id": "PU9LDUXvj4l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents_embeddings(query_embedding, k=10):\n",
        "    query_embedding_list = query_embedding.tolist()\n",
        "\n",
        "    results = db_file.query(\n",
        "        query_embeddings=[query_embedding_list],\n",
        "\n",
        "        n_results=k)\n",
        "    return results"
      ],
      "metadata": {
        "id": "1Gf7G2DU1hyz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1AMya0p4jnD"
      },
      "source": [
        "Ø§Ù„Ø¹Ø¨ Ø¨Ù‚ÙŠ Ù…Ù† Ø§ÙˆÙ„ Ù‡Ù†Ø§ ðŸ˜²\n",
        "Prompt\n",
        "query\n",
        "model\n",
        "output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igcRsKVGMgyF",
        "outputId": "21cd308a-f1dd-4ccb-a045-8b7e4e947aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contentadditional_kwargsresponse_metadatatypenameidexampletool_callsinvalid_tool_callsusage_metadata"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.schema import HumanMessage  # Import HumanMessage\n",
        "\n",
        "# Set the GROQ API Key as an environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize the ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    api_key=os.getenv(\"GROQ_API_KEY\"),  # Retrieve the API key from the environment variable\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "# Create a list of messages using the HumanMessage class\n",
        "messages = [\n",
        "    HumanMessage(content=\"Hello\")\n",
        "]\n",
        "\n",
        "# Sending a message to the model\n",
        "response = llm.invoke(messages)  # Use invoke instead of call\n",
        "\n",
        "# Handle streaming response\n",
        "for chunk in response:\n",
        "    # Assuming the structure is (content, metadata), modify according to actual structure\n",
        "    content = chunk[0] if isinstance(chunk, tuple) else chunk\n",
        "    print(content or \"\", end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLz-Nju8PM1f",
        "outputId": "4d1c112c-7136-4605-89fb-94466fcc594b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF-zgGCrPD4u"
      },
      "outputs": [],
      "source": [
        "#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"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen RAG"
      ],
      "metadata": {
        "id": "TOmxl6qb1vEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "m_HoYqI_Pb6q"
      },
      "outputs": [],
      "source": [
        "qna_template = \"\\n\".join([\n",
        "    \"Act as a knowledgeable law professor. Analyze the provided legal context and respond to the subsequent question with thoroughness and clarity. If the information needed to answer the question is not present in the context, respond with 'NO ANSWER IS AVAILABLE.'\",\n",
        "    \"give the answer in details with Arabic accent \",\n",
        "\n",
        "    \"### Context\",\n",
        "    \"{context}\",\n",
        "\n",
        "    \"### Question\",\n",
        "    \"{question}\",\n",
        "\n",
        "    \"### Answer:\",\n",
        "])\n",
        "\n",
        "qna_prompt = PromptTemplate(\n",
        "    template=qna_template,\n",
        "    input_variables=['context', 'question'],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "stuff_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=qna_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query\n",
        "query = ''''\n",
        "\n",
        "Ù…Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§ØªØŸ\n",
        "\n",
        "'''\n",
        "\n",
        "# Generate the embedding for the query\n",
        "query_embedding = base_model.encode(query)\n",
        "\n",
        "# Perform similarity search in ChromaDB\n",
        "similar_documents = retrieve_documents_embeddings(query_embedding, k=7)\n",
        "\n"
      ],
      "metadata": {
        "id": "ADw5t1W1Zwod"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = similar_documents['documents'][0]\n",
        "metadatas = similar_documents['metadatas'][0]\n",
        "\n",
        "from langchain.schema import Document\n",
        "formatted_documents = [\n",
        "    Document(page_content=doc, metadata=meta)\n",
        "    for doc, meta in zip(documents, metadatas)\n",
        "]"
      ],
      "metadata": {
        "id": "14vy04IW0D6h"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OaMsiKJdJec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae9b130-390e-4c65-9ae5-b6ccb415417b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer: {'output_text': 'Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª ØªÙƒÙ…Ù† ÙÙŠ ÙƒÙˆÙ†Ù‡ Ø§Ù„ÙˆØ³ÙŠÙ„Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ Ù„Ù„Ø¯Ø§Ø¦Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ù‚Ù‡ ÙÙŠ Ø­Ø§Ù„Ø© Ø±ÙØ¶ Ø§Ù„Ù…Ø¯ÙŠÙ† Ø·ÙˆØ§Ø¹ÙŠØ© Ø§Ù„ÙˆÙØ§Ø¡ Ø¨Ù…Ø§ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„ØªØ²Ø§Ù…Ø§Øª. ÙŠØ¹ØªØ¨Ø± Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ Ø¢Ø®Ø± ÙˆØ§Ø®Ø·Ø± Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØµØ±Ø§Ø¹ Ø­ÙˆÙ„ Ø§Ù„Ø­Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¯Ø§Ø¦Ù† ÙˆØ§Ù„Ù…Ø¯ÙŠÙ†ØŒ ÙˆÙŠØªØ·Ù„Ø¨ ØªØ¯Ø®Ù„ Ø§Ù„Ù…Ø´Ø±Ø¹ Ù„ØªÙ†Ø¸ÙŠÙ… Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø®ØµÙˆÙ…Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù…Ø§ ÙŠØ­Ù‚Ù‚ Ø§Ù„Ù…ØµØ§Ù„Ø­ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø£Ø·Ø±Ø§Ù Ù‡Ø°Ù‡ Ø§Ù„Ø®ØµÙˆÙ…Ø©ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„ØµØ§Ù„Ø­ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ø­Ø§ÙØ¸Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø¯ÙˆÙ„Ø©.\\n\\nØ£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠÙ…ÙƒÙ† ØªÙ„Ø®ÙŠØµÙ‡Ø§ ÙÙŠ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ§Ù„ÙŠØ©:\\n\\n1.  **Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ‚**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø£Ùˆ Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ© Ø§Ù„Ø«Ø§Ø¨ØªØ© ÙÙŠ Ø§Ù„Ø³Ù†Ø¯Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ©ØŒ ÙˆØ¶Ù…Ø§Ù† ØªÙ†ÙÙŠØ°Ù‡Ø§ ÙÙŠ Ø¸Ù„ ØºÙŠØ§Ø¨ Ø¥Ø±Ø§Ø¯Ø© Ø§Ù„Ù…Ø¯ÙŠÙ†.\\n2.  **ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø¨ÙŠÙ† Ø£Ø·Ø±Ø§Ù Ø§Ù„Ø®ØµÙˆÙ…Ø©ØŒ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¶Ù…Ø§Ù† Ø­ØµÙˆÙ„ ÙƒÙ„ Ø°ÙŠ Ø­Ù‚ Ø¹Ù„Ù‰ Ø­Ù‚Ù‡.\\n3.  **Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ³Ø§Ù‡Ù… ÙÙŠ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŒ ÙˆÙŠØ±Ø³Ø® Ø§Ù„Ø«Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£ÙØ±Ø§Ø¯ ÙˆØ§Ù„Ù…Ø¤Ø³Ø³Ø§Øª.\\n4.  **Ø¶Ù…Ø§Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ø­ÙƒØ§Ù…**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¶Ù…Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©ØŒ ÙˆÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø­Ù…Ø§ÙŠØ© Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø¯Ø§Ø¦Ù†ÙŠÙ†.\\n5.  **ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ§Ø·Ù†Ø©**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ§Ø·Ù†Ø©ØŒ ÙˆØ¶Ù…Ø§Ù† Ø­ØµÙˆÙ„ ÙƒÙ„ ÙØ±Ø¯ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚Ù‡ ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©.\\n\\nÙÙŠ Ø§Ù„Ø®Ù„Ø§ØµØ©ØŒ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹ØªØ¨Ø± Ø±ÙƒÙŠØ²Ø© Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§ØªØŒ ÙˆÙŠÙ„Ø¹Ø¨ Ø¯ÙˆØ±Ø§Ù‹ Ø­ÙŠÙˆÙŠØ§Ù‹ ÙÙŠ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙˆØ¶Ù…Ø§Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.'}\n"
          ]
        }
      ],
      "source": [
        "# Use the stuff chain to generate the answer\n",
        "answer = stuff_chain(\n",
        "    {\n",
        "        \"input_documents\": formatted_documents,  # Pass the search results directly\n",
        "        \"question\": query  # Pass the original query\n",
        "    },\n",
        "    return_only_outputs=True,  # Return only the output from the chain\n",
        ")\n",
        "\n",
        "# Print the answer\n",
        "print(\"Generated Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the output text from the answer\n",
        "output_text = answer['output_text']\n",
        "\n",
        "# Print the formatted answer\n",
        "print(\"Generated Answer:\")\n",
        "print(\"=\" * 50)  # Separator line\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgf9zScIcNMt",
        "outputId": "c5c5e086-956c-45d2-a8db-9694a027d603"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer:\n",
            "==================================================\n",
            "Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª ØªÙƒÙ…Ù† ÙÙŠ ÙƒÙˆÙ†Ù‡ Ø§Ù„ÙˆØ³ÙŠÙ„Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ Ù„Ù„Ø¯Ø§Ø¦Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ù‚Ù‡ ÙÙŠ Ø­Ø§Ù„Ø© Ø±ÙØ¶ Ø§Ù„Ù…Ø¯ÙŠÙ† Ø·ÙˆØ§Ø¹ÙŠØ© Ø§Ù„ÙˆÙØ§Ø¡ Ø¨Ù…Ø§ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„ØªØ²Ø§Ù…Ø§Øª. ÙŠØ¹ØªØ¨Ø± Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ Ø¢Ø®Ø± ÙˆØ§Ø®Ø·Ø± Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØµØ±Ø§Ø¹ Ø­ÙˆÙ„ Ø§Ù„Ø­Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¯Ø§Ø¦Ù† ÙˆØ§Ù„Ù…Ø¯ÙŠÙ†ØŒ ÙˆÙŠØªØ·Ù„Ø¨ ØªØ¯Ø®Ù„ Ø§Ù„Ù…Ø´Ø±Ø¹ Ù„ØªÙ†Ø¸ÙŠÙ… Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø®ØµÙˆÙ…Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù…Ø§ ÙŠØ­Ù‚Ù‚ Ø§Ù„Ù…ØµØ§Ù„Ø­ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø£Ø·Ø±Ø§Ù Ù‡Ø°Ù‡ Ø§Ù„Ø®ØµÙˆÙ…Ø©ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„ØµØ§Ù„Ø­ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ø­Ø§ÙØ¸Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø¯ÙˆÙ„Ø©.\n",
            "\n",
            "Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠÙ…ÙƒÙ† ØªÙ„Ø®ÙŠØµÙ‡Ø§ ÙÙŠ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ§Ù„ÙŠØ©:\n",
            "\n",
            "1.  **Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ‚**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø£Ùˆ Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ© Ø§Ù„Ø«Ø§Ø¨ØªØ© ÙÙŠ Ø§Ù„Ø³Ù†Ø¯Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ©ØŒ ÙˆØ¶Ù…Ø§Ù† ØªÙ†ÙÙŠØ°Ù‡Ø§ ÙÙŠ Ø¸Ù„ ØºÙŠØ§Ø¨ Ø¥Ø±Ø§Ø¯Ø© Ø§Ù„Ù…Ø¯ÙŠÙ†.\n",
            "2.  **ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© Ø¨ÙŠÙ† Ø£Ø·Ø±Ø§Ù Ø§Ù„Ø®ØµÙˆÙ…Ø©ØŒ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¶Ù…Ø§Ù† Ø­ØµÙˆÙ„ ÙƒÙ„ Ø°ÙŠ Ø­Ù‚ Ø¹Ù„Ù‰ Ø­Ù‚Ù‡.\n",
            "3.  **Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ³Ø§Ù‡Ù… ÙÙŠ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŒ ÙˆÙŠØ±Ø³Ø® Ø§Ù„Ø«Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£ÙØ±Ø§Ø¯ ÙˆØ§Ù„Ù…Ø¤Ø³Ø³Ø§Øª.\n",
            "4.  **Ø¶Ù…Ø§Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ø­ÙƒØ§Ù…**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¶Ù…Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©ØŒ ÙˆÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø­Ù…Ø§ÙŠØ© Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø¯Ø§Ø¦Ù†ÙŠÙ†.\n",
            "5.  **ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ§Ø·Ù†Ø©**: Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ§Ø·Ù†Ø©ØŒ ÙˆØ¶Ù…Ø§Ù† Ø­ØµÙˆÙ„ ÙƒÙ„ ÙØ±Ø¯ Ø¹Ù„Ù‰ Ø­Ù‚ÙˆÙ‚Ù‡ ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©.\n",
            "\n",
            "ÙÙŠ Ø§Ù„Ø®Ù„Ø§ØµØ©ØŒ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹ØªØ¨Ø± Ø±ÙƒÙŠØ²Ø© Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§ØªØŒ ÙˆÙŠÙ„Ø¹Ø¨ Ø¯ÙˆØ±Ø§Ù‹ Ø­ÙŠÙˆÙŠØ§Ù‹ ÙÙŠ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ù‚ÙˆÙ‚ ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙˆØ¶Ù…Ø§Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothetical Document Embeddings\n"
      ],
      "metadata": {
        "id": "n8njMCAFskq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query\n",
        "query_hyde = ''''\n",
        "\n",
        "Ù…Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§ØªØŸ\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "1kA5q1GM03AW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothetical Document Embeddings\n",
        "\n",
        "\n",
        "# reusing the question and vectorstore as earlier\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# HyDE document genration prompt\n",
        "template_hyde = \"\\n\".join([\n",
        "    \"Act as a Law professor from egypt. Analyze the provided law question and respond to the subsequent question with thoroughness and clarity.\",\n",
        "    \"give the answer in details in arabic only \",\n",
        "\n",
        "\n",
        "    \"### Question\",\n",
        "    \"{question}\",\n",
        "\n",
        "    \"### Answer:\",\n",
        "])\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template_hyde)\n",
        "\n",
        "# generating a hypothetical document based on the user input\n",
        "hypothetical_document = llm.invoke(prompt_hyde.format(question=query_hyde))\n"
      ],
      "metadata": {
        "id": "oyFfU_RH0iiG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate the embedding for the query\n",
        "hypothetical_document_embedding = base_model.encode(hypothetical_document.content)\n",
        "\n",
        "# Perform similarity search in ChromaDB\n",
        "similar_documents_hyde = retrieve_documents_embeddings(hypothetical_document_embedding, k=5)\n",
        "\n",
        "\n",
        "\n",
        "documents_hyde = similar_documents_hyde['documents'][0]\n",
        "metadatas_hyde = similar_documents_hyde['metadatas'][0]\n",
        "\n",
        "from langchain.schema import Document\n",
        "formatted_documents_hyde = [\n",
        "    Document(page_content=doc, metadata=meta)\n",
        "    for doc, meta in zip(documents_hyde, metadatas_hyde)\n",
        "]\n"
      ],
      "metadata": {
        "id": "4wLIGiyi0jfW"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = stuff_chain(\n",
        "        {\n",
        "            \"input_documents\": formatted_documents_hyde ,\n",
        "            \"question\": query\n",
        "        },\n",
        "        return_only_outputs=True\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the output text from the answer\n",
        "output_text = answer['output_text']\n",
        "\n",
        "# Print the formatted answer\n",
        "print(\"Generated Answer:\")\n",
        "print(\"=\" * 50)  # Separator line\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s7XjcIjsiLQ",
        "outputId": "1290cb1b-9eb7-44da-8a6a-40475bc98b0b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer:\n",
            "==================================================\n",
            "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…ØŒ Ø£Ø³ØªØ§Ø°ÙŠ Ø§Ù„Ø¹Ø²ÙŠØ²ØŒ Ø³Ø£Ø¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª.\n",
            "\n",
            "Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ·Ù„Ø¨ Ù…Ù†Ø§ Ø£Ù† Ù†ÙƒØ´Ù Ø¹Ù† Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§ØªØŒ ÙˆÙŠØªØ¹Ù„Ù‚ Ø°Ù„Ùƒ Ø¨Ù…ÙÙ‡ÙˆÙ… ÙˆÙ‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ.\n",
            "\n",
            "Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠØŒ Ø£Ø³ØªØ§Ø°ÙŠØŒ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª ØªÙƒÙ…Ù† ÙÙŠÙ…Ø§ ÙŠØ£ØªÙŠ:\n",
            "\n",
            "Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ Ù‡Ùˆ Ø§Ù„ÙˆØ³ÙŠÙ„Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ Ù„Ù„Ø¯Ø§Ø¦Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ù‚Ù‡ ÙÙŠ Ø­Ø§Ù„Ø© Ø±ÙØ¶ Ø§Ù„Ù…Ø¯ÙŠÙ† Ø·ÙˆØ§Ø¹ÙŠØ© Ø§Ù„ÙˆÙØ§Ø¡ Ø¨Ù…Ø§ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø§Ù„ØªØ²Ø§Ù…Ø§Øª. ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¶Ù…Ù† Ù„Ù„ÙØ±Ø¯ Ø­Ù‚Ù‡ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø§ Ù„Ù‡ Ù…Ù† Ø­Ù‚ÙˆÙ‚Ù‡ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.\n",
            "\n",
            "Ø«Ø§Ù†ÙŠØ§Ù‹ØŒ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹ØªØ¨Ø± Ø§Ù„ÙˆØ³ÙŠÙ„Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ Ø¥Ø¹Ù…Ø§Ù„ Ø¬Ø²Ø§Ø¡ Ù…Ø®Ø§Ù„ÙØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ§Ù‚Ø¹. ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¶Ù…Ù† Ù„Ù„Ù…Ø¬ØªÙ…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©.\n",
            "\n",
            "Ø«Ø§Ù„Ø«Ø§Ù‹ØŒ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¶Ù…Ù† Ù„Ù„Ù…Ø¯ÙŠÙ† Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ø³ØªØºÙ„Ø§Ù„ Ø§Ù„Ø¯Ø§Ø¦Ù† Ù„Ù‡. ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¶Ù…Ù† Ù„Ù„Ù…Ø¯ÙŠÙ† Ø­Ù‚ÙˆÙ‚Ù‡ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.\n",
            "\n",
            "Ø±Ø§Ø¨Ø¹Ø§Ù‹ØŒ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¹ØªØ¨Ø± ÙˆØ³ÙŠÙ„Ø© Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø¯ÙˆÙ„Ø©. ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙŠØ¶Ù…Ù† Ù„Ù„Ù…Ø¬ØªÙ…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©.\n",
            "\n",
            "ÙˆØ£Ø®ÙŠØ±Ø§Ù‹ØŒ Ø£Ø³ØªØ§Ø°ÙŠØŒ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ØªÙƒÙ…Ù† ÙÙŠ Ø¯ÙˆØ±Ù‡Ø§ ÙÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ù…ØµØ§Ù„Ø­ Ø§Ù„Ø®Ø§ØµØ© Ù„Ø£Ø·Ø±Ø§Ù Ø§Ù„Ø®ØµÙˆÙ…Ø©ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„ØµØ§Ù„Ø­ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ø­Ø§ÙØ¸Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø¯ÙˆÙ„Ø©.\n",
            "\n",
            "Ø®ØªØ§Ù…Ø§Ù‹ØŒ Ø£Ø³ØªØ§Ø°ÙŠ Ø§Ù„Ø¹Ø²ÙŠØ²ØŒ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¬Ø¨Ø±ÙŠ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª ØªÙƒÙ…Ù† ÙÙŠ Ø¶Ù…Ø§Ù† Ø­Ù‚ÙˆÙ‚ Ø§Ù„ÙØ±Ø¯ ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŒ ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ØŒ ÙˆØ­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø¯ÙŠÙ† Ù…Ù† Ø§Ø³ØªØºÙ„Ø§Ù„ Ø§Ù„Ø¯Ø§Ø¦Ù† Ù„Ù‡ØŒ ÙˆØªÙƒØ§Ù…Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø¯ÙˆÙ„Ø©.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0KN3yKgQgSv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Data/Ø­Ù‚ÙˆÙ‚ /Ø¹ÙŠÙ† Ø´Ù…Ø³ /Ø±Ø§Ø¨Ø¹Ù‡/CSV/Ø§Ù„ØªÙ†ÙÙŠØ°_Ø§Ù„Ø¬Ø¨Ø±ÙŠ_Ø§Ù„Ø¬Ø²Ø¡_Ø§Ù„Ø«Ø§Ù†ÙŠ_Ø§Ù„Ù‚Ø§ÙŠÙ”Ù…_Ø¨Ø§Ù„ØªØ¯Ø±ÙŠØ³_Ø¯_Ø³ÙŠØ¯_Ø§Ù”Ø¨Ùˆ_Ø³Ø±ÙŠØ¹.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "yqfUa44XiTW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "print(\"Context:\\n\", df.loc[2, \"context\"])\n",
        "\n",
        "print(\"Question:\\n\", df.loc[2, \"question\"])\n",
        "print(\"Answer:\\n\", df.loc[2, \"answer\"])\n"
      ],
      "metadata": {
        "id": "U987gaAciaN3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}