{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed28112003/Muffakir/blob/main/Notebooks/Frozen_RAG_%26_HyDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "bY5XOMBuA8Uv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bWmGd91a030"
      },
      "outputs": [],
      "source": [
        "!pip install  transformers\n",
        "!pip install langchain\n",
        "!pip install -U langchain-community\n",
        "!pip install unstructured\n",
        "!pip install \"unstructured[pdf]\"\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install LangChain -q\n",
        "!pip install langchain transformers\n",
        "!pip install chromadb\n",
        "!pip install sentence-transformers\n",
        "!pip install rank-bm25 sentence-transformers\n",
        "!pip uninstall nltk -y\n",
        "!pip install nltk\n",
        "!pip install openai\n",
        "!pip install groq\n",
        "!pip install langchain-groq\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pzXFntYq8pys"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "import re\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.schema import HumanMessage  # Import HumanMessage\n",
        "import gradio as gr\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import Document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gt5ufyTkPPq"
      },
      "source": [
        "# Load TXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2-3BXNRVxMkI"
      },
      "outputs": [],
      "source": [
        "data_loader = DirectoryLoader(\"/content/Data\",\n",
        "                             glob=\"*.txt\",\n",
        "                             show_progress=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter_recursive = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # Adjust chunk size based on content length\n",
        "    chunk_overlap=0,  # No overlap needed\n",
        "    separators=[\"Page number\"]  # Split at answer or next question\n",
        ")"
      ],
      "metadata": {
        "id": "G2UdWq4rqFxn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents from the specified directory\n",
        "data = data_loader.load_and_split(text_splitter=text_splitter_recursive) # Load documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaWMKcinqIAg",
        "outputId": "ffeac1b4-c401-4f07-c42c-40b13dd8b6d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:18<00:00, 13.83s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Documents : {len(data)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLa3P6NRRNOr",
        "outputId": "a29cc779-2831-4e2f-f9bd-d138638e00e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents : 3391.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove \"Page number \" from each chunk using regex\n",
        "for i in range(len(data)):\n",
        "  data[i].page_content = re.sub(r\"Page number: \\d+\", \"\", data[i].page_content)"
      ],
      "metadata": {
        "id": "tZnvUF1JrUS2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTfS8mm05aif"
      },
      "source": [
        "# Embeddding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxwZM_CvE0Qy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the base model\n",
        "base_model = SentenceTransformer('intfloat/multilingual-e5-large')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "path = \"/content/DB\"\n",
        "client = chromadb.PersistentClient(path=path)\n",
        "\n",
        "db_file = client.get_or_create_collection(name='Book',\n",
        "                                          metadata={\"hnsw:space\": \"cosine\"})"
      ],
      "metadata": {
        "id": "Xg9WQfTx7Dst"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(texts):\n",
        "    return base_model.encode(texts)\n",
        "\n",
        "i = 0\n",
        "for chunk in data:\n",
        "    embeddings = embed([chunk.page_content])  # Get embeddings using the base model\n",
        "    db_file.add(\n",
        "        documents=[chunk.page_content],\n",
        "        metadatas=[chunk.metadata],\n",
        "        ids=[f\"chunk_{i}\"],\n",
        "        embeddings=embeddings  # Pass the embeddings directly\n",
        "    )\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "Bf7HWs3qumwJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_file.get(ids=[\"chunk_2\"])"
      ],
      "metadata": {
        "id": "PU9LDUXvj4l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents_embeddings(query_embedding, k=10):\n",
        "    query_embedding_list = query_embedding.tolist()\n",
        "\n",
        "    results = db_file.query(\n",
        "        query_embeddings=[query_embedding_list],\n",
        "\n",
        "        n_results=k)\n",
        "    return results"
      ],
      "metadata": {
        "id": "1Gf7G2DU1hyz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1AMya0p4jnD"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igcRsKVGMgyF",
        "outputId": "96437c7e-50d4-4a68-be32-af5cceda7c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contentadditional_kwargsresponse_metadatatypenameidexampletool_callsinvalid_tool_callsusage_metadata"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Set the GROQ API Key as an environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize the ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    api_key=os.getenv(\"GROQ_API_KEY\"),  # Retrieve the API key from the environment variable\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "# Create a list of messages using the HumanMessage class\n",
        "messages = [\n",
        "    HumanMessage(content=\"Hello\")\n",
        "]\n",
        "\n",
        "# Sending a message to the model\n",
        "response = llm.invoke(messages)  # Use invoke instead of call\n",
        "\n",
        "# Handle streaming response\n",
        "for chunk in response:\n",
        "    # Assuming the structure is (content, metadata), modify according to actual structure\n",
        "    content = chunk[0] if isinstance(chunk, tuple) else chunk\n",
        "    print(content or \"\", end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLz-Nju8PM1f",
        "outputId": "52994ebd-baaa-4247-8ea1-2b0acb79a996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kF-zgGCrPD4u"
      },
      "outputs": [],
      "source": [
        "#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"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen RAG"
      ],
      "metadata": {
        "id": "TOmxl6qb1vEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "m_HoYqI_Pb6q"
      },
      "outputs": [],
      "source": [
        "qna_template = \"\\n\".join([\n",
        "    \"Act as a knowledgeable law professor. Analyze the provided legal context and respond to the subsequent question with thoroughness and clarity. If the information needed to answer the question is not present in the context, respond with 'NO ANSWER IS AVAILABLE.'\",\n",
        "    \"give the answer in details with Arabic accent \",\n",
        "\n",
        "    \"### Context\",\n",
        "    \"{context}\",\n",
        "\n",
        "    \"### Question\",\n",
        "    \"{question}\",\n",
        "\n",
        "    \"### Answer:\",\n",
        "])\n",
        "\n",
        "qna_prompt = PromptTemplate(\n",
        "    template=qna_template,\n",
        "    input_variables=['context', 'question'],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "stuff_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=qna_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query\n",
        "query = ''''\n",
        "\n",
        "ما أهمية التنفيذ الجبري في قانون المرافعات؟\n",
        "\n",
        "'''\n",
        "\n",
        "# Generate the embedding for the query\n",
        "query_embedding = base_model.encode(query)\n",
        "\n",
        "# Perform similarity search in ChromaDB\n",
        "similar_documents = retrieve_documents_embeddings(query_embedding, k=7)\n",
        "\n"
      ],
      "metadata": {
        "id": "ADw5t1W1Zwod"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = similar_documents['documents'][0]\n",
        "metadatas = similar_documents['metadatas'][0]\n",
        "\n",
        "from langchain.schema import Document\n",
        "formatted_documents = [\n",
        "    Document(page_content=doc, metadata=meta)\n",
        "    for doc, meta in zip(documents, metadatas)\n",
        "]"
      ],
      "metadata": {
        "id": "14vy04IW0D6h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaMsiKJdJec4"
      },
      "outputs": [],
      "source": [
        "# Use the stuff chain to generate the answer\n",
        "answer = stuff_chain(\n",
        "    {\n",
        "        \"input_documents\": formatted_documents,  # Pass the search results directly\n",
        "        \"question\": query  # Pass the original query\n",
        "    },\n",
        "    return_only_outputs=True,  # Return only the output from the chain\n",
        ")\n",
        "\n",
        "# Print the answer\n",
        "print(\"Generated Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the output text from the answer\n",
        "output_text = answer['output_text']\n",
        "\n",
        "# Print the formatted answer\n",
        "print(\"Generated Answer:\")\n",
        "print(\"=\" * 50)  # Separator line\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "id": "qgf9zScIcNMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Frozen RAG"
      ],
      "metadata": {
        "id": "uW2rbR3z8ou7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make the above gradio code also print the metadats\n",
        "\n",
        "def generate_answer(query):\n",
        "    # Generate the embedding for the query\n",
        "    query_embedding = base_model.encode(query)\n",
        "\n",
        "    # Perform similarity search in ChromaDB\n",
        "    similar_documents = retrieve_documents_embeddings(query_embedding, k=7)\n",
        "\n",
        "    documents = similar_documents['documents'][0]\n",
        "    metadatas = similar_documents['metadatas'][0]\n",
        "\n",
        "    formatted_documents = [\n",
        "        Document(page_content=doc, metadata=meta)\n",
        "        for doc, meta in zip(documents, metadatas)\n",
        "    ]\n",
        "    # Use the stuff chain to generate the answer\n",
        "    answer = stuff_chain(\n",
        "        {\n",
        "            \"input_documents\": formatted_documents,  # Pass the search results directly\n",
        "            \"question\": query  # Pass the original query\n",
        "        },\n",
        "        return_only_outputs=True,  # Return only the output from the chain\n",
        "    )\n",
        "\n",
        "    # Get the output text from the answer\n",
        "    output_text = answer['output_text']\n",
        "\n",
        "    # Prepare similar documents for display\n",
        "    similar_docs_str = \"\\n\".join([f\"**Document {i+1}:**\\n{doc}\\n**Metadata:** {meta}\" for i, (doc, meta) in enumerate(zip(documents, metadatas))])\n",
        "\n",
        "    return output_text, similar_docs_str\n",
        "\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_answer,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter your query here...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer\"),\n",
        "        gr.Textbox(label=\"Similar Documents with Metadata\")\n",
        "    ],\n",
        "\n",
        "        examples=[\n",
        "        \"ما أهمية التنفيذ الجبري في قانون المرافعات؟\",\n",
        "        \"ما هي قواعد الاختصاص القضائي في القانون المصري؟\",\n",
        "    ],\n",
        "    title=\"Muffakir\",\n",
        "    description=\"Ask your questions\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "XTW3vIGu7qVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothetical Document Embeddings\n"
      ],
      "metadata": {
        "id": "n8njMCAFskq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query\n",
        "query_hyde = ''''\n",
        "\n",
        "ما أهمية التنفيذ الجبري في قانون المرافعات؟\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "1kA5q1GM03AW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothetical Document Embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# HyDE document genration prompt\n",
        "template_hyde = \"\\n\".join([\n",
        "    \"Act as a Law professor from egypt. Analyze the provided law question and respond to the subsequent question with thoroughness and clarity.\",\n",
        "    \"give the answer in details in arabic only \",\n",
        "\n",
        "\n",
        "    \"### Question\",\n",
        "    \"{question}\",\n",
        "\n",
        "    \"### Answer:\",\n",
        "])\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template_hyde)\n",
        "\n",
        "# generating a hypothetical document based on the user input\n",
        "hypothetical_document = llm.invoke(prompt_hyde.format(question=query_hyde))\n"
      ],
      "metadata": {
        "id": "oyFfU_RH0iiG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate the embedding for the query\n",
        "hypothetical_document_embedding = base_model.encode(hypothetical_document.content)\n",
        "\n",
        "# Perform similarity search in ChromaDB\n",
        "similar_documents_hyde = retrieve_documents_embeddings(hypothetical_document_embedding, k=5)\n",
        "\n",
        "\n",
        "\n",
        "documents_hyde = similar_documents_hyde['documents'][0]\n",
        "metadatas_hyde = similar_documents_hyde['metadatas'][0]\n",
        "\n",
        "from langchain.schema import Document\n",
        "formatted_documents_hyde = [\n",
        "    Document(page_content=doc, metadata=meta)\n",
        "    for doc, meta in zip(documents_hyde, metadatas_hyde)\n",
        "]\n"
      ],
      "metadata": {
        "id": "4wLIGiyi0jfW"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = stuff_chain(\n",
        "        {\n",
        "            \"input_documents\": formatted_documents_hyde ,\n",
        "            \"question\": query\n",
        "        },\n",
        "        return_only_outputs=True\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the output text from the answer\n",
        "output_text = answer['output_text']\n",
        "\n",
        "# Print the formatted answer\n",
        "print(\"Generated Answer:\")\n",
        "print(\"=\" * 50)  # Separator line\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "id": "6s7XjcIjsiLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio HyDE"
      ],
      "metadata": {
        "id": "ud1RYUFBAwYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# HyDE document generation prompt\n",
        "template_hyde = \"\\n\".join([\n",
        "    \"Act as a Law professor from Egypt. Analyze the provided law question and respond to the subsequent question with thoroughness and clarity.\",\n",
        "    \"give the answer in details in arabic only.\",\n",
        "\n",
        "    \"### Question\",\n",
        "    \"{question}\",\n",
        "\n",
        "    \"### Answer:\",\n",
        "])\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template_hyde)\n",
        "\n",
        "# Function to handle chatbot interaction\n",
        "def law_chatbot(query):\n",
        "    try:\n",
        "        # Generate hypothetical document\n",
        "        hypothetical_document = llm.invoke(prompt_hyde.format(question=query))\n",
        "\n",
        "        # Generate embedding for the hypothetical document\n",
        "        hypothetical_document_embedding = base_model.encode(hypothetical_document.content)\n",
        "\n",
        "        # Perform similarity search in ChromaDB\n",
        "        similar_documents_hyde = retrieve_documents_embeddings(hypothetical_document_embedding, k=5)\n",
        "\n",
        "        documents_hyde = similar_documents_hyde['documents'][0]\n",
        "        metadatas_hyde = similar_documents_hyde['metadatas'][0]\n",
        "\n",
        "        # Format documents for input into the stuff chain\n",
        "        formatted_documents_hyde = [\n",
        "            Document(page_content=doc, metadata=meta)\n",
        "            for doc, meta in zip(documents_hyde, metadatas_hyde)\n",
        "        ]\n",
        "\n",
        "        # Use the chain to generate the answer\n",
        "        answer = stuff_chain(\n",
        "            {\n",
        "                \"input_documents\": formatted_documents_hyde,\n",
        "                \"question\": query\n",
        "            },\n",
        "            return_only_outputs=True\n",
        "        )\n",
        "\n",
        "        # Extract the output text\n",
        "        output_text = answer['output_text']\n",
        "\n",
        "        # Prepare similar documents and metadata for separate display\n",
        "        documents_display = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(documents_hyde)])\n",
        "        metadata_display = \"\\n\\n\".join([f\"Metadata {i+1}:\\n{meta}\" for i, meta in enumerate(metadatas_hyde)])\n",
        "\n",
        "        return output_text, documents_display, metadata_display\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\", \"\", \"\"\n",
        "\n",
        "# Gradio interface with multiple output boxes\n",
        "iface = gr.Interface(\n",
        "    fn=law_chatbot,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Chatbot Answer\", lines=10),\n",
        "        gr.Textbox(label=\"Similar Documents\", lines=10),\n",
        "        gr.Textbox(label=\"Metadata\", lines=10),\n",
        "    ],\n",
        "\n",
        "\n",
        "        examples=[\n",
        "        \"ما أهمية التنفيذ الجبري في قانون المرافعات؟\",\n",
        "        \"ما هي قواعد الاختصاص القضائي في القانون المصري؟\",\n",
        "    ],\n",
        "\n",
        "    title=\"Muffakir HyDE \",\n",
        "    description=\"Ask any question\",\n",
        "\n",
        ")\n",
        "\n",
        "# Launch the chatbot\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "hEHego1e-7WL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7gt5ufyTkPPq"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}